{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "class HyperParameterTuning:\n",
    "    \"\"\"\n",
    "    Contains methods to return a pipeline object and a dictionary containing\n",
    "    classifier parameters\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, classifier, vectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            classifier (One of 6 sklearn classifier objects): 'logreg', 'svm', 'nb',\n",
    "                                                              'knn', 'xgboost', 'randomforests'\n",
    "            vectorizer (CountVectorizer or TfidfVectorizer): Type of vector space model\n",
    "        Returns:\n",
    "            pipeline (sklearn pipeline object): Returns a pipeline object which is used\n",
    "                                                by GridSearchCV\n",
    "            model_params[self.classifier] (dict): Returns a dictionary of parameters\n",
    "                                                  for the specified type of classifier\n",
    "        \"\"\"\n",
    "\n",
    "        self.classifier = classifier\n",
    "        self.vectorizer = vectorizer\n",
    "\n",
    "    def get_pipeline(self):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            classifier (One of 6 sklearn classifier objects): 'logreg', 'svm', 'nb',\n",
    "                                                              'knn', 'xgboost', 'randomforests'\n",
    "            vectorizer (CountVectorizer or TfidfVectorizer): Type of vector space model\n",
    "        Returns:\n",
    "            pipeline (sklearn pipeline object): Returns a pipeline object which is\n",
    "                                                used by GridSearchCV\n",
    "            model_params[self.classifier] (dict): Returns a dictionary of parameters\n",
    "                                                  for the specified type of classifier\n",
    "        \"\"\"\n",
    "\n",
    "        classifier_objects = {'nb': MultinomialNB()}\n",
    "        pipeline = Pipeline([('vect', self.vectorizer),\n",
    "                             ('clf', classifier_objects[self.classifier])])\n",
    "\n",
    "        return pipeline\n",
    "\n",
    "    def get_params(self):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            self\n",
    "        Returns:\n",
    "            model_params[self.classifier] (dict): Returns a dictionary of parameters for the\n",
    "                                                  specified type of classifier\n",
    "        \"\"\"\n",
    "        model_params = {'nb': {'clf__alpha': (0, 1), 'clf__fit_prior': (True, False)}}\n",
    "        return model_params[self.classifier]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "\n",
    "class VectorSpace:\n",
    "    \"\"\"\n",
    "    Creates vector space model for training data with specifications of weighting factors,\n",
    "    reductions, stop words and ngram combination\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train, weighting_factor=None, reduction=None,\n",
    "                 stop_words=None, ngrams=None):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            train (Pandas DataFrame): the training data\n",
    "            weighting_factor (Optional argument, None by default, str otherwise)):\n",
    "                                can take 'TF' or 'IDF'\n",
    "            reduction (Optional argument, None by default, str otherwise):\n",
    "                                can take 'stem' or 'lemmatize'\n",
    "            stop_words (Optional argument, None by default, str otherwise):\n",
    "                                can take 'english'\n",
    "            ngrams (Optional argument, None by default, tuple otherwise):\n",
    "                                can take (1, 1), (1, 2) or (2,2)\n",
    "        Returns:\n",
    "            vectorizer (CountVectorizer or TfidfVectorizer object)\n",
    "            train (Pandas DataFrame): the training data with reduction applied (if any)\n",
    "        \"\"\"\n",
    "        print(\"Parameters recieved: \", weighting_factor, reduction, stop_words, ngrams)\n",
    "        self.train = train\n",
    "        self.weighting_factor = weighting_factor\n",
    "        self.stop_words = stop_words\n",
    "        self.ngrams = ngrams\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def lemmatize_sentences(self, sentence):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sentence (str): A single sentence from a Pandas DataFrame\n",
    "                            and applied the reduction (if any)\n",
    "        Returns:\n",
    "            lemmatized_tokens (str): A single sentence from a Pandas DataFrame\n",
    "                            with the reduction applied (if any)\n",
    "        \"\"\"\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = sentence.split()\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        return ' '.join(lemmatized_tokens)\n",
    "\n",
    "    def stem_sentences(self, sentence):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sentence (str): A single sentence from a Pandas DataFrame\n",
    "                            and applied the reduction (if any)\n",
    "        Returns:\n",
    "            stemmed_tokens (str): A single sentence from a Pandas DataFrame\n",
    "                            with the reduction applied (if any)\n",
    "        \"\"\"\n",
    "\n",
    "        porter_stemmer = PorterStemmer()\n",
    "        tokens = sentence.split()\n",
    "        stemmed_tokens = [porter_stemmer.stem(token) for token in tokens]\n",
    "        return ' '.join(stemmed_tokens)\n",
    "\n",
    "    def apply_reduction(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            self\n",
    "        Returns:\n",
    "            train (Pandas DataFrame): Returns the train data instance with the reduction\n",
    "                                      applied (if any)\n",
    "        \"\"\"\n",
    "\n",
    "        if self.reduction == 'stem':\n",
    "            print(\"Performing reduction: stemming\")\n",
    "            self.train = self.train.apply(self.stem_sentences)\n",
    "        elif self.reduction == 'lemmatize':\n",
    "            print(\"Performing reduction: lemmatization\")\n",
    "            self.train = self.train.apply(self.lemmatize_sentences)\n",
    "        return self.train\n",
    "\n",
    "    def tf_vectorizer(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            self\n",
    "        Returns:\n",
    "            vectorizer (CountVectorizer object)\n",
    "            train (Pandas DataFrame): the training data with reduction applied (if any)\n",
    "        \"\"\"\n",
    "\n",
    "        self.train = self.apply_reduction()\n",
    "        print(\"Returning CountVectorizer object with parameters: \", self.stop_words, self.ngrams)\n",
    "        vectorizer = CountVectorizer(stop_words=self.stop_words, ngram_range=self.ngrams)\n",
    "        return vectorizer, self.train\n",
    "\n",
    "    def tfidf_vectorizer(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            self\n",
    "        Returns:\n",
    "            vectorizer (TfidfVectorizer object)\n",
    "            train (Pandas DataFrame): the training data with reduction applied (if any)\n",
    "        \"\"\"\n",
    "\n",
    "        self.train = self.apply_reduction()\n",
    "        print(\"Returning TfidfVectorizer object with parameters: \", self.stop_words, self.ngrams)\n",
    "        vectorizer = TfidfVectorizer(stop_words=self.stop_words, ngram_range=self.ngrams)\n",
    "        return vectorizer, self.train\n",
    "\n",
    "    def create_vec_space(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            self\n",
    "        Returns:\n",
    "            vectorizer (TfidfVectorizer object)\n",
    "        \"\"\"\n",
    "\n",
    "        if self.weighting_factor == 'TF':\n",
    "            return self.tf_vectorizer()\n",
    "        return self.tfidf_vectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tune_fit_model(transformed_data, parameter_desc, train):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        transformed_data (list of lists): Every list, has two components.\n",
    "                                          1. vectorizer: which is either CountVectorizer or\n",
    "                                             TfidfVectorizer\n",
    "                                          2. reduced_data: has the training set with\n",
    "                                             reduction such as stemming or lemmatization\n",
    "                                             applied\n",
    "        parameter_desc (list of lists): For each list in transformed_data, the corresponding list\n",
    "                                        in the same index have description about the data so it's\n",
    "                                        easy to plot graphs or sort once model accuracies have been\n",
    "                                        calculated\n",
    "        train (Pandas DataFrame): contains the training data including the labels\n",
    "    Returns:\n",
    "         None\n",
    "    \"\"\"\n",
    "\n",
    "    count = 0\n",
    "    params = []\n",
    "    for vectorizer, reduced_data in transformed_data:\n",
    "\n",
    "        for classifier in {'nb'}:\n",
    "            print(\"Data: \", parameter_desc[count])\n",
    "            print(\"Classfier: \", classifier)\n",
    "            x_train, x_test, y_train, y_test = train_test_split(reduced_data,\n",
    "                                                                train['Score'],\n",
    "                                                                stratify=train['Score'],\n",
    "                                                                test_size=0.15)\n",
    "            hyperparam_instance = HyperParameterTuning(classifier, vectorizer)\n",
    "            search = GridSearchCV(hyperparam_instance.get_pipeline(),\n",
    "                                  param_grid=hyperparam_instance.get_params(),\n",
    "                                  cv=5, scoring='accuracy',\n",
    "                                  n_jobs=-1)\n",
    "            search.fit(x_train, y_train)\n",
    "            y_pred = search.predict(x_test)\n",
    "            print(\"Validation accuracy\", accuracy_score(y_test, y_pred))\n",
    "            print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "            print(search.best_params_)\n",
    "            param = [parameter_desc[count], classifier, search.best_score_,\n",
    "                     accuracy_score(y_test, y_pred), search.best_params_]\n",
    "            params.append(param)\n",
    "            print(param)\n",
    "            print()\n",
    "        count += 1\n",
    "\n",
    "    param_df = pd.DataFrame(params, columns=['Dataset', 'Classifier', 'Training accuracy',\n",
    "                                             'Validation accuracy', 'Classifier object'])\n",
    "    \n",
    "    print(param_df)\n",
    "    #param_df.to_csv(\"/Users/abhishekbabuji/Desktop/ModelPerformances.csv\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    The main function that reads in the train.csv from my local file system\n",
    "    \"\"\"\n",
    "\n",
    "    train = pd.read_csv(data_location)\n",
    "    datasets = [[train['Text']], ['TF', 'TFIDF'], ['stem', 'lemmatize', None], ['english', None],\n",
    "                [(1, 1), (1, 2), (2, 2)]]\n",
    "    dataset_combination = list(itertools.product(*datasets))\n",
    "\n",
    "    transformed_data = []\n",
    "    parameter_desc = []\n",
    "    count = 0\n",
    "\n",
    "    for dataset_params in dataset_combination:\n",
    "        count += 1\n",
    "        print(\"Transformed data no. \", count)\n",
    "        model = VectorSpace(dataset_params[0], dataset_params[1], dataset_params[2],\n",
    "                            dataset_params[3], dataset_params[4])\n",
    "        vectorizer, reduced_data = model.create_vec_space()\n",
    "\n",
    "        transformed_data.append([vectorizer, reduced_data])\n",
    "        parameter_desc.append([dataset_params[1], dataset_params[2],\n",
    "                               dataset_params[3], dataset_params[4]])\n",
    "        print(\"Vector space transformation applied with parameters: \",\n",
    "              dataset_params[1], dataset_params[2], dataset_params[3], dataset_params[4])\n",
    "        print()\n",
    "\n",
    "    tune_fit_model(transformed_data, parameter_desc, train)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    data_location = \"\" #Enter data location here\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
