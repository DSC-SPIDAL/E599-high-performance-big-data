{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import gensim\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend import tensorflow_backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "bucket='sagemaker-hpc-yelp'\n",
    "data_key = 'cleaned_HPC.csv'\n",
    "data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "\n",
    "\n",
    "role = get_execution_role()\n",
    "bucket='sagemaker-hpc-yelp'\n",
    "embd_key = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "embd_location = 's3://{}/{}'.format(bucket, embd_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_small_dataset(file_path, file_type):\n",
    "\n",
    "    \"\"\"\n",
    "    Only use for small JSON or CSV files. If it is a large dataset, then you'll need\n",
    "    to appropriately read only specific columns or in chunks to save space.\n",
    "    Args:\n",
    "        file_path (str): Path in the local directory\n",
    "        file_type (str): Can be 'JSON' or 'CSV'\n",
    "    Returns:\n",
    "        data_frame (pandas.DataFrame)\n",
    "    \"\"\"\n",
    "\n",
    "    if file_type == 'JSON':\n",
    "        data_frame = pd.read_json(file_path, lines=True)\n",
    "\n",
    "    else:\n",
    "        data_frame = pd.read_csv(file_path)\n",
    "\n",
    "    return data_frame\n",
    "\n",
    "def read_large_dataset(file_path, file_type, column_names):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        file_path (str): Path in the local directory\n",
    "        file_type (str): Can be 'JSON' or 'CSV'\n",
    "        column_names (list): List of columns to be read\n",
    "    Returns:\n",
    "        data_frame (pandas.DataFrame)\n",
    "    \"\"\"\n",
    "\n",
    "    empty_list = [] #List to push in all the relevant rows and columns\n",
    "\n",
    "    if file_type == 'JSON':\n",
    "        with open(file_path, 'r') as file_opened:\n",
    "            for line in file_opened:\n",
    "                data = json.loads(line)\n",
    "                empty_list.append([data[column_names[0]],\n",
    "                                   data[column_names[1]],\n",
    "                                   data[column_names[2]]])\n",
    "\n",
    "        data_frame = pd.DataFrame(empty_list)\n",
    "        data_frame.columns = column_names\n",
    "        return data_frame\n",
    "\n",
    "\n",
    "    data_frame = pd.read_csv(file_path)\n",
    "    return data_frame\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        text (str): Each row of a DataFrame as text\n",
    "    Returns:\n",
    "        text (str): cleaned test\n",
    "    \"\"\"\n",
    "    ## Remove puncuation\n",
    "    text = text.translate(string.punctuation)\n",
    "\n",
    "    ## Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    ## Remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    ## Clean the text: Self explanatory\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def create_embedding_index(model_path):\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model_path (str): Path to Word2vec model in the local file system\n",
    "    Returns:\n",
    "        embedding_index (dict): A dictionary of vectors representing word embedding for each word\n",
    "    \"\"\"\n",
    "\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(model_path,\n",
    "                                                            binary=True)\n",
    "\n",
    "    words = model.index2word\n",
    "    embedding_index = dict()\n",
    "\n",
    "    for word in words:\n",
    "        embedding_index[word] = model[word]\n",
    "\n",
    "    return embedding_index\n",
    "\n",
    "def create_padded_sequence(vocabulary_size, maxlen, data_frame, text_column):\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        vocabulary_size (int): Top `vocabulary_size` to be considered\n",
    "        maxlen (int): Maximum length of the sequence\n",
    "        data_frame (pandas.DataFrame): Yelp pizza DataFrame containing reviews\n",
    "    Returns:\n",
    "        data (np.array): Our input data converted to sequence\n",
    "        tokenizer (keras.preprocessing.text.Tokenizer): Keras tokenizer object\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=vocabulary_size)\n",
    "    tokenizer.fit_on_texts(data_frame[text_column])\n",
    "    sequences = tokenizer.texts_to_sequences(data_frame[text_column])\n",
    "    data = pad_sequences(sequences, maxlen)\n",
    "\n",
    "    return data, tokenizer\n",
    "\n",
    "def create_embedding_matrix(vocabulary_size, num_dimensions, tokenizer, embeddings_index):\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        vocabulary_size (int): Top `vocabulary_size` words\n",
    "                               being considered whose word vectors are being extracted\n",
    "        num_dimensions (int): Word vector dimensions\n",
    "        tokenizer (keras.preprocessing.text.Tokenizer): Keras tokenizer object\n",
    "        embeddings_index (dict): Dictionary containing indices of words\n",
    "    Returns:\n",
    "        embedding_matrix (np.array): This will be the input\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    embedding_matrix = np.zeros((vocabulary_size, num_dimensions))\n",
    "\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "\n",
    "        if index > vocabulary_size - 1:\n",
    "            break\n",
    "        else:\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[index] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def fit_lstms(num_units, embedding_weights, num_epochs, fit_data, ):\n",
    "\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        num_units (int): Number of LSTM units\n",
    "        weights (numpy.array): The embedding matrix\n",
    "        epochs (int): Number of gradient descent iterations\n",
    "        fit_data (dict): Dictionary containing the training and testing data\n",
    "    Returns:\n",
    "        model_glove (Keras model): Can be used to predict on unseen data\n",
    "    \"\"\"\n",
    "    model_glove = Sequential()\n",
    "    model_glove.add(Embedding(50000,\n",
    "                              300,\n",
    "                              input_length=60,\n",
    "                              weights=embedding_weights,\n",
    "                              trainable=False))\n",
    "\n",
    "    model_glove.add(Dropout(0.5))\n",
    "    model_glove.add(Conv1D(64, 5, activation='relu'))\n",
    "    model_glove.add(MaxPooling1D(pool_size=4))\n",
    "    model_glove.add(Bidirectional(LSTM(num_units)))\n",
    "    model_glove.add(Dense(3, activation='softmax'))\n",
    "    model_glove.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model_glove.summary())\n",
    "\n",
    "    model_glove.fit(fit_data['x_train'],\n",
    "                    fit_data['y_train'],\n",
    "                    validation_data=(fit_data['x_test'], fit_data['y_test']),\n",
    "                    epochs=num_epochs,\n",
    "                    batch_size=1024)\n",
    "    \n",
    "    y_pred = model_glove.predict(fit_data['x_test'])\n",
    "\n",
    "    ex = [tuple(y_s) for y_s in fit_data['y_test']]\n",
    "    \n",
    "    dictionary = dict(zip(ex, fit_data['y_test'].argmax(axis=1)))\n",
    "    \n",
    "    print(classification_report(fit_data['y_test'].argmax(axis=1), y_pred.argmax(axis=1)))\n",
    "            \n",
    "    return model_glove, dictionary\n",
    "\n",
    "def main():\n",
    "\n",
    "    \"\"\"\n",
    "    Main method divided into parts\n",
    "    \"\"\"\n",
    "    #print(\"Reading in business.json\")\n",
    "    \n",
    "    #df = pd.read_csv(\"/Users/abhishekbabuji/Downloads/amazon-fine-food-reviews/Reviews.csv\")\n",
    "    #df = df.loc[(df['HelpfulnessDenominator'] != 0) & (df['HelpfulnessDenominator'] != 1)]\n",
    "    #df = df[['Score', 'Text']]\n",
    "    #df['Score'] = df['Score'].apply({1: 'Bad', 2: 'Bad', 3: 'Average', 4: 'Good', 5: 'Good'}.get)\n",
    "    \n",
    "\n",
    "    #matrix = metrics.confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "\n",
    "    #print(\"Done!\")\n",
    "    #print()\n",
    "    #print(\"Cleaning text...\")\n",
    "    \n",
    "    #df['Text'] = df['Text'].map(lambda x: clean_text(x))\n",
    "    #print(\"Done!\")\n",
    "    #print()\n",
    "    data_location = \"\" #Enter path to CSV file\n",
    "    df = pd.read_csv(data_location)\n",
    "    word2vec_model_path = embd_location\n",
    "    \n",
    "    print(\"Creating embedding index\")\n",
    "    embedding_index = create_embedding_index(word2vec_model_path)\n",
    "\n",
    "    vocabulary_size = 50000\n",
    "    maxlen = 60\n",
    "    num_dimensions = 300\n",
    "    print(\"Creating sequence from text reviews\")\n",
    "    data, tokenizer = create_padded_sequence(vocabulary_size, maxlen, df, 'Text')\n",
    "    print(\"Fitting embedding matrix\")\n",
    "    embedding_matrix = create_embedding_matrix(vocabulary_size,\n",
    "                                               num_dimensions,\n",
    "                                               tokenizer,\n",
    "                                               embedding_index)\n",
    "\n",
    "    print(\"Done!\")\n",
    "    print()\n",
    "    #Part 4: Train test split, and one hot encoding the labels\n",
    "    print(\"Creating one hot encoded labels\")\n",
    "    encoder = LabelBinarizer()\n",
    "    \n",
    "    one_hot_label = encoder.fit_transform(np.array(df[['Score']]))\n",
    "    \n",
    "    print(encoder.inverse_transform(one_hot_label))\n",
    "    \n",
    "    print(one_hot_label)\n",
    "    \n",
    "    labels = [tuple(label) for label in one_hot_label]\n",
    "    labels_encoded = dict(zip(labels, encoder.inverse_transform(one_hot_label)))\n",
    "    \n",
    "    print(labels_encoded)\n",
    "    \n",
    "    \n",
    "    print(\"Splitting sequences into train/test\")\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, one_hot_label,\n",
    "                                                        random_state=42,\n",
    "                                                        stratify=one_hot_label,\n",
    "                                                        test_size=0.15)\n",
    "\n",
    "    print(\"Fitting Keras Model\")\n",
    "    #Part 5: Fitting our Keras models\n",
    "    data_to_fit = {'x_train': x_train,\n",
    "                   'x_test': x_test,\n",
    "                   'y_train': y_train,\n",
    "                   'y_test': y_test}\n",
    "\n",
    "    startTime = datetime.now()\n",
    "    model_ten_units = fit_lstms(num_units=100, #Change number of units here\n",
    "                                embedding_weights=[embedding_matrix],\n",
    "                                num_epochs=100, #Change number of epochs here\n",
    "                                fit_data=data_to_fit)\n",
    "    \n",
    "    print(datetime.now() - startTime)\n",
    "    print()\n",
    "    print(model_ten_units)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    \n",
    "\n",
    "    with tf.Session(config=tf.ConfigProto(\n",
    "                        intra_op_parallelism_threads=2)) as sess: #Change thread num here\n",
    "        K.set_session(sess)\n",
    "        \n",
    "        #import os\n",
    "        #print(os.system('top -b -n 1'))\n",
    "        main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
